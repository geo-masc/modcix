---
title: "MODCiX Accuracy Assessment"
output: github_document
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../docs", output_file = file.path(dirname(inputFile), 'index.html')) })
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Accuracy Assessment of Mowing Events

This notebook evaluates the accuracy of predicted mowing events against reference data. The steps include data loading, preprocessing, filtering, and computing accuracy metrics.

For data privacy reasons, we are not allowed to share the exact reference data. We therefore provide dummy datasets that simulate reference events for two regions and corresponding predictions from two teams.

### 1. Load Libraries and Define Constants

We start by loading the necessary libraries and defining constants.

```{r load-libraries}
# Load necessary libraries
library(tidyverse)
library(lubridate)
```
```{r define-constants}
# Define constants and paths
TOLERANCE <- 12  # Temporal distance between reference and prediction to be considered correct
VALID_MOWING_RANGE <- 75:300  # Valid range for mowing dates (15th March to 27th October)
EVENT_MIN_DIFFERENCE <- 15  # Minimum difference in days between consecutive mowing events

REFERENCE_DATA_PATH <- "../data/reference_data_dummy.csv"  # Path to reference data
RESULTS_PATH <- "../data/results_data_dummy.csv"  # Path to results data
```

### 2. Load and Prepare Reference Data

We load the reference data check that all mowing events are within our defined valid mowing period between March 15 and October 27 (```VALID_MOWING_RANGE```). The dates of the reference mowing events are in the day of the year (DOY) format.

```{r load-reference-data}
# Load reference data and preprocess it
reference_data <- read_csv(REFERENCE_DATA_PATH) %>%
  filter(Date_ref %in% VALID_MOWING_RANGE)

head(reference_data)
```

We then check the temporal distance of consecutive mowing events on the same field. Events that are closer to each other than our defined threshold ```EVENT_MIN_DIFFERENCE``` are discarded.

```{r}
# check temporal distance between reference events and remove if < threshold
diff_reference_dates <- reference_data %>%
  group_by(MOD_ID, Year) %>%
  arrange(MOD_ID, Year, Date_ref) %>%
  mutate(difference = c(NA, diff(Date_ref)))

invalid_reference_dates <- diff_reference_dates %>%
  filter(difference < EVENT_MIN_DIFFERENCE) %>%
  distinct(MOD_ID, Year)

reference_data_cleaned <- reference_data %>%
  anti_join(invalid_reference_dates)

head(reference_data_cleaned)
```

## 4. Load and Prepare Results Data

We load the harmonized predictions and discard predictions outside the valid mowing period (```VALID_MOWING_RANGE```). Regions and year combinations that are not present in the reference data are also filtered out.

```{r}
# Load and preprocess results data
results <- readr::read_csv(RESULTS_PATH) %>%
  distinct() %>%
  filter(Date_pred %in% VALID_MOWING_RANGE) %>%
  drop_na() %>%
  semi_join(reference_data_cleaned, by = c("Region", "Year"))
```


## 5. Joining Predictions and Reference Data

For each reference event, we now join all predictions of the respective field and year. This is performed separately for each group. We then calculate the temproal distance in days for each joined prediction and keep only the nearest prediction.

The valid cuts (true positives; TP) are then defined as the matches between reference events and predictions with less than ```TOLERANCE``` days absolute difference. All other events are counted as false positives (FP).


```{r}
# Calculate absolute errors between reference and predicted dates
joined_cuts <- reference_data_cleaned %>%
  left_join(results, by = c("MOD_ID", "Year", "Region")) %>%
  mutate(difference = abs(Date_ref - Date_pred)) %>%
  group_by(MOD_ID, Year, Date_ref, Group) %>%
  filter(difference == min(difference)) %>%
  slice(1) %>%
  ungroup()

# Filter valid predictions within tolerance
valid_cuts <- joined_cuts %>%
  filter(difference <= TOLERANCE)
```


## 6. Accuracy Assessment

We then count the number of:

* true reference (T)
  * all positive events in the reference data
* true positives (TP)
  * correctly predicted events
* total positives (P)
  * all predicted events wether correct or not
* and false positives (FP)
  * incorrect predictions

```{r}
# Calculate the number of reference cuts per region and year
t <- reference_data %>%
  count(Region, Year, name = "T")

# Calculate the number of true positives per region and group
tp <- valid_cuts %>% 
  count(Group, Region, Year, Method, Data, name = "TP")

# Calculate the number of positives per region and group
p <- results %>% 
  count(Group, Region, Year, Method, Data, name = "P")

# Combine counts of positives and negatives
acc_assess_values <- p %>%
  left_join(t, by = c("Region", "Year")) %>%
  left_join(tp, by = c("Group", "Region", "Year", "Method", "Data")) %>%
  mutate(across(c("P", "T", "TP"), ~ replace_na(.x, 0))) %>%
  mutate(FP = P - TP)
```

We then also summarize the accuracy values per region to to be able to analyze differences.

```{r}
# Summarize accuracy values across all regions and years

acc_assess_values_all <- acc_assess_values %>%
  mutate(Year = as.character(Year)) %>%
  # merge all regions and set to all
  bind_rows(mutate(., Region = "All")) %>%
  # merge all years and set to all
  bind_rows(mutate(., Year = "All")) %>%
  group_by(Group, Region, Year, Method, Data) %>%
  # summary only relevant for Region = All and Year = All
  summarise(across(P:FP, sum), .groups = "drop")
```

Finally, we can calculate the following metrics:

* Precision: the ratio of correctly predicted positive observations to the total predicted positives.

* Recall: the ratio of correctly predicted positive observations to all observations in the actual class.

* F1 Score: the harmonic mean of Precision and Recall. Therefore, this score takes both false positives and false negatives into account.

```{r}
# calculate accuracy metrics
acc_assess_metrics <- acc_assess_values_all %>%
  mutate(Recall = TP / T) %>%
  mutate(Precision = TP / P) %>%
  mutate(Precision = replace_na(Precision, 0)) %>%
  mutate(F1 = (2 * Precision * Recall) / (Precision + Recall)) %>%
  mutate(F1 = replace_na(F1, 0))
```

## 7. Visualizing the results

We can now visualize the accuracy assessment to highlight differences in the predictions of the different groups and between specific years and regions.

###### 1. Differences between the groups taking into account all regions and years.

```{r, echo=FALSE}
acc_assess_metrics %>%
  filter(Year == "All") %>%
  filter(Region == "All") %>%
  ggplot(aes(x = Group)) +
  geom_segment(aes(y = Recall, yend = Precision, xend = Group)) +
  geom_point(aes(y = Recall, color = "Recall", shape = "Recall"), size = 2.5) +
  geom_point(aes(y = Precision, color = "Precision", shape = "Precision"), size = 2.5) +
  geom_point(aes(y = F1, color = "F1-Score", shape = "F1-Score"), size = 2.5) +
  scale_color_manual(name = "Metric", breaks = c("F1-Score", "Precision", "Recall"), values = ggthemes::colorblind_pal()(3)[1:3]) +
  scale_shape_manual(name = "Metric", breaks = c("F1-Score", "Precision", "Recall"), values = c(15,16,16)) +
  theme_minimal() +
  labs(y = "Metric") +
  theme(legend.title = element_blank())
```

###### 2. Differences between the groups separated by regions.

```{r, echo=FALSE}
acc_assess_metrics %>% 
  filter(Region != "All") %>% 
  filter(Year == "All") %>%
  mutate(Region_n = paste0(Region, " (n= ", T, ")")) %>%
  ggplot(aes(x = Group)) +
  geom_segment(aes(y = Recall, yend = Precision, xend = Group)) +
  geom_point(aes(y = Recall, color = "Recall", shape = "Recall"), size = 2.5) +
  geom_point(aes(y = Precision, color = "Precision", shape = "Precision"), size = 2.5) +
  geom_point(aes(y = F1, color = "F1-Score", shape = "F1-Score"), size = 2.5) +
  scale_color_manual(name = "Metric", breaks = c("F1-Score", "Precision", "Recall"), values = ggthemes::colorblind_pal()(3)[1:3]) +
  scale_shape_manual(name = "Metric", breaks = c("F1-Score", "Precision", "Recall"), values = c(15,16,16)) +
  facet_wrap(~Region_n) +
  theme_minimal() +
  labs(y = "Metric [-]") +
  theme(legend.title = element_blank())
```

###### 3. Differences between the groups separated by years.

```{r, echo=FALSE}
acc_assess_metrics %>%
  filter(Region == "All") %>%
  mutate(Year_n = paste0(Year, " (n= ", T, ")")) %>%
  ggplot(aes(x = Group)) +
  geom_segment(aes(y = Recall, yend = Precision, xend = Group)) +
  geom_point(aes(y = Recall, color = "Recall", shape = "Recall"), size = 2.5) +
  geom_point(aes(y = Precision, color = "Precision", shape = "Precision"), size = 2.5) +
  geom_point(aes(y = F1, color = "F1-Score", shape = "F1-Score"), size = 2.5) +
  facet_wrap(~Year_n) +
  scale_color_manual(name = "Metric", breaks = c("F1-Score", "Precision", "Recall"), values = ggthemes::colorblind_pal()(3)[1:3]) +
  scale_shape_manual(name = "Metric", breaks = c("F1-Score", "Precision", "Recall"), values = c(15,16,16)) +
  theme_minimal() +
  labs(y = "Metric [-]") +
  theme(legend.title = element_blank())
```